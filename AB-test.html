<!DOCTYPE html>
<html lang="en">

        <!-- Head/CSS referral -->
<head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width">
        <title>Imre Deak: Data Analyst</title>
        <link rel="stylesheet" type="text/css" href="style4.css">
</head>
        
<a name="top"></a>
<body> <div class="container"> <div class="content">
        
<h1> <a href="https://www.imredeak.com">Return to home page</a> </h1>
        <br><hr style="color: white">

        <!-- Main body -->

<h3> Project #2 <br><br> Measuring the statistical significance of an ad campaign (A/B testing) </h3> <!-- title -->
<hr style="color: white">

<h3> File Repository </h3> <!-- Section 0.5 -->
        
 <!-- Data: .csv, analysis: .py -->  
<p>
<a href="Project2/marketing_AB.csv" class="white-link">Raw Data (.csv)</a>
        <br>
<a href="Project2/AB_testing.py" class="white-link">Python Script (.py)</a>
        <br>
<a href="Project2/PythonScript.txt" class="white-link">Python Script (.txt)</a> </p> <br><hr style="color: white">
        
<h3> Utility of A/B testing </h3> <!-- Section 1 -->

<p>

A/B testing is a statistical evaluation of two pre-determined design choices randomly assigned to a pool of users, usually in the context of front-end development and marketing analysis.
The objective of these tests is simply to verify which design choice, if any, leads to improvement in a desired metric, such as click rates, retention or purchases/subscriptions.
It is also possible to analyse more than two design choices (colours of a subscription button) or a chain of independent design choices (<i>colour → size → font</i> of a subscription button), 
though this would be referred to as multivariate analysis. 
        <br><br>
This project aimed to develop my automation skills in the context of an A/B analysis on the significance of a realistic ad campaign (ie. marketing analysis). I have set up several functions which will 
correctly pick out the most appropriate test (more on this later) based on the results of several assumption checks - in order to determine the significance of the ad campaign if the data receives an update.
        <br><br>
Vigorous A/B testing is especially crucial in the context of marketing analysis over classic user interface examples, as rolling out an ad campaign involves significantly higher costs (e.g. planning, 
asset design, potential contracts) than updating a website, therefore making the correct* data-driven decision is particularly vital in order to maximise firm revenue and minimise unnecessary losses. 
<br> (* with posterior beliefs based on available data)
        
</p>
  
<h3> Data description & visualisation </h3> <!-- Section 2 -->

<p> 

With 588,101 observations of unique individuals, this dataset is designed to capture changes in consumer purchasing patterns (converted variable, i.e. did the individual buy or disregard the product) 
at various levels of ad exposure. 
The categorisation of a control group (individuals who have not seen any ads) and the experimental group (individuals exposed to the ads at least once) creates the foundation of the A/B significance analysis. 
 <br><br>
Further variables enable us to explore the nuances of customer behaviour: </p>
        
<ul>
<li>The total number of times the ads were exposed to the individual</li>
<li>The day of the week when the individual was exposed to the greatest number of ads</li>
<li>The hour at which the individual was exposed to the greatest number of ads</li>
</ul>
        
<p>
Elementary univariate visualisation of: 
<a href="Project2/UnivariateCountPlots.png">categoric variables</a> & <a href="Project2/FilteredHistogramBoxplot.png">non-categoric 'total ads' variable</a> 
(filtered out extreme outliers for spread clarity in latter). <br><br>

Bivariate visualisation of the <a href="Project2/AvgNumberOfAdsConverted.png">distribution of the number of ads seen</a> by those have purchased the product and those who haven't<br>
(filtered extreme values for spread clarity). Based on this boxplot, if we find the ad campaign to make significant changes to sales rates, individuals should be targeted to face ad exposure about 26 times. 
        <br><br>

Mondays (3.3%), Tuesdays (3.0%) and Wednesdays (2.5%) were the days with the highest proportion of successful transactions, and the late afternoon and evening hours of the day (15:00 - 20:00) received the 
greatest proportion of successful traffic ranging from 2.6% to 3.1% of converted customers.
        
</p>
  
<h3> A/B Testing between the two groups </h3> <!-- Section 3 -->

<p> 

<u>Test 1: Chi-Square Test</u> <br><br>

It's important to isolate each variable against the customer conversions, and by running the Chi-Square test, we can outrule the likelihood of the association being determined by random chance. 
Having conducted the test for each variable at a 1% significance level, a statistically significant relationship between the variables and conversion rates was found in all cases. <br><br>

<u>Test 2: A/B Test Function Nest</u> <br><br>

With a sample size as large as this data is, it's a no-brainer to run non-parametric tests in the real world, as it omits testing for the conditions necessary for parametrics tests 
(normal distribution & equal variances assumptions). However, my goal for this analysis is to be able to use this code no matter what variation of the data gets funneled into it 
(for instance, a particular subset of the same variables), therefore I have created a chain of functions using various tests from the SciPy package to automatically select the most appropriate 
statistical tests regardless. 
        <br><br>

The first function tests for the assumptions required for parametric tests: the Shapiro-Wilkinson test for normal distribution (5% alpha), and Levene's test for measuring the equality of variances. 
        <br><br>
        
Based on the result of these tests, the second function picks either the optimal parametric test to determine the statistical impact of the ad campaign, the t-test (compares data through the mean), 
or its optimal non-parametric alternative, the Mann-Whitney U test (compares data through the median). 
        <br><br>

In this case, both assumption tests defied the null hypothesis (p < 0.05), meaning the data is not normally distributed and that there is heteroskedasticity in my data (variance across groups are not equal). 
        <br><br>

Therefore, it's only appropriate to run the non-parametric Mann-Whitney U test to determine the significance of the results of the ad campaign. With a p-value of 0.0, there is <u>extremely strong 
statistical evidence</u> to say there is a difference between the results of the control and treatment group, implying that the ad campaign does indeed successfully increase the proportion of people who will 
buy the product.

</p>

<h3> Conclusion </h3> <!-- Section 4/5 -->

<p>
        
This project demonstrates the application of automation in A/B testing to evaluate the significance of a realistic ad campaign, with a focus on identifying statistically supported insights 
that drive decision-making in marketing. Using a dataset of 588,101 unique observations, the project explores consumer behaviour patterns, highlighted key trends, and automated the selection of 
optimal statistical tests to ensure robustness.

Findings revealed a significant relationship between ad exposure and conversion rates, with late afternoons and early evenings proving particularly effective for successful transactions. 
Non-parametric testing via the Mann-Whitney U test confirmed the ad campaign's positive impact on purchase rates, providing sufficient evidence to guide marketing strategy.
        
</p>
  
                <!-- End /template.html -->
        
<a href="#top">{Back to Top}</a>
</div> </div> </body>
</html>
